version: "3.8"

services:
  ### Dependencies
  pg:
    image: elestio/postgres:15
    restart: always
    ports:
      - "172.17.0.1:20011:5432"
    volumes:
      - ./pg_data:/VAR/LIB/POSTGRESQL/DATA
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}

  redis-standalone:
    image: elestio/redis:7.0
    restart: always
    ports:
      - "172.17.0.1:16379:6379"

  migration_runner:
    image: rust:latest
    command: "bash -c 'cargo install diesel_cli --no-default-features --features postgres && diesel migration --database-url postgres://$${DATABASE_USER}:$${DATABASE_PASSWORD}@$${DATABASE_HOST}:$${DATABASE_PORT}/$${DATABASE_NAME} run'"
    working_dir: /app
    volumes:
      - ./:/app
    environment:
      - DATABASE_USER=${POSTGRES_USER}
      - DATABASE_PASSWORD=${POSTGRES_PASSWORD}
      - DATABASE_HOST=${DATABASE_HOST}
      - DATABASE_PORT=${DATABASE_PORT}
      - DATABASE_NAME=${POSTGRES_DB}

  ### Application services
  hyperswitch-server:
    image: juspaydotin/hyperswitch-router:standalone
    restart: always
    command: /local/bin/router -f /local/config/docker_compose.toml
    ports:
      - "172.17.0.1:10742:8080"
    volumes:
      - ./config:/local/config
      - ./files:/local/bin/files
    labels:
      logs: "promtail"

  hyperswitch-producer:
    image: juspaydotin/hyperswitch-producer:standalone
    restart: always
    command: /local/bin/scheduler -f /local/config/docker_compose.toml
    profiles:
      - scheduler
    volumes:
      - ./config:/local/config
    environment:
      - SCHEDULER_FLOW=producer
    depends_on:
      - hyperswitch-consumer
    labels:
      logs: "promtail"

  hyperswitch-consumer:
    image: juspaydotin/hyperswitch-consumer:standalone
    restart: always
    command: /local/bin/scheduler -f /local/config/docker_compose.toml
    profiles:
      - scheduler
    volumes:
      - ./config:/local/config
    environment:
      - SCHEDULER_FLOW=consumer
    depends_on:
      - hyperswitch-server
    labels:
      logs: "promtail"

  hyperswitch-drainer:
    image: juspaydotin/hyperswitch-drainer:standalone
    restart: always
    command: /local/bin/drainer -f /local/config/docker_compose.toml
    deploy:
      replicas: ${DRAINER_INSTANCE_COUNT:-1}
    profiles:
      - full_kv
    volumes:
      - ./config:/local/config
    depends_on:
      - hyperswitch-server
    labels:
      logs: "promtail"

  ### Clustered Redis setup
  redis-cluster:
    image: elestio/redis:7.0
    restart: always
    deploy:
      replicas: ${REDIS_CLUSTER_COUNT:-3}
    command: redis-server /usr/local/etc/redis/redis.conf
    profiles:
      - clustered_redis
    volumes:
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379"
      - "16379"

  redis-init:
    image: elestio/redis:7.0
    restart: always
    profiles:
      - clustered_redis
    depends_on:
      - redis-cluster
    command: "bash -c 'export COUNT=${REDIS_CLUSTER_COUNT:-3}

      \ if [ $$COUNT -lt 3 ]

      \ then

      \ echo \"Minimum 3 nodes are needed for redis cluster\"

      \ exit 1

      \ fi

      \ HOSTS=\"\"

      \ for ((c=1; c<=$$COUNT;c++))

      \ do

      \ NODE=$COMPOSE_PROJECT_NAME-redis-cluster-$$c:6379

      \ echo $$NODE

      \ HOSTS=\"$$HOSTS $$NODE\"

      \ done

      \ echo Creating a cluster with $$HOSTS

      \ redis-cli --cluster create $$HOSTS --cluster-yes

      \ '"

  ### Monitoring
  grafana:
    image: grafana/grafana:latest
    restart: always
    ports:
      - "172.17.0.1:64972:3000"
    profiles:
      - monitoring
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_BASIC_ENABLED=false
    volumes:
      - ./config/grafana.ini:/etc/grafana/grafana.ini
      - ./config/grafana-datasource.yaml:/etc/grafana/provisioning/datasources/datasource.yml

  promtail:
    image: grafana/promtail:latest
    restart: always
    volumes:
      - ./logs:/var/log/router
      - ./config:/etc/promtail
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/promtail.yaml
    profiles:
      - monitoring

  loki:
    image: grafana/loki:latest
    restart: always
    ports:
      - "3100"
    command: -config.file=/etc/loki/loki.yaml
    profiles:
      - monitoring
    volumes:
      - ./config:/etc/loki

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    restart: always
    command: --config=/etc/otel-collector.yaml
    profiles:
      - monitoring
    volumes:
      - ./config/otel-collector.yaml:/etc/otel-collector.yaml
    ports:
      - "4317"
      - "8888"
      - "8889"

  prometheus:
    image: prom/prometheus:latest
    restart: always
    profiles:
      - monitoring
    volumes:
      - ./config/prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090"
  tempo:
    image: grafana/tempo:latest
    restart: always
    command: -config.file=/etc/tempo.yaml
    volumes:
      - ./config/tempo.yaml:/etc/tempo.yaml
    profiles:
      - monitoring
    ports:
      - "3200" # tempo
      - "4317" # otlp grpc

  redis-insight:
    image: redislabs/redisinsight:latest
    restart: always
    profiles:
      - full_kv
    ports:
      - "172.17.0.1:11392:8001"
    volumes:
      - ./redisinsight_store:/db

  kafka0:
    image: confluentinc/cp-kafka:7.0.5
    restart: always
    hostname: kafka0
    ports:
      - 9092:9092
      - 9093
      - 9997
      - 29092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka0:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka0:29093"
      KAFKA_LISTENERS: "PLAINTEXT://kafka0:29092,CONTROLLER://kafka0:29093,PLAINTEXT_HOST://0.0.0.0:9092"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
      JMX_PORT: 9997
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka0 -Dcom.sun.management.jmxremote.rmi.port=9997
    profiles:
      - analytics
    volumes:
      - ./monitoring/kafka-script.sh:/tmp/update_run.sh
    command: 'bash -c ''if [ ! -f /tmp/update_run.sh ]; then echo "ERROR: Did you forget the update_run.sh file that came with this docker-compose.yml file?" && exit 1 ; else /tmp/update_run.sh && /etc/confluent/docker/run ; fi'''

  # Kafka UI for debugging kafka queues
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    restart: always
    ports:
      - 172.17.0.1:63773:8080
    depends_on:
      - kafka0
    profiles:
      - analytics
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka0:29092
      KAFKA_CLUSTERS_0_JMXPORT: 9997

  clickhouse-server:
    image: clickhouse/clickhouse-server:23.5
    restart: always
    ports:
      - "9000"
      - "8123:8123"
    profiles:
      - analytics
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
